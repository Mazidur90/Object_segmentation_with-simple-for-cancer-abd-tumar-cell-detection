{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrington\\AppData\\Local\\Temp\\ipykernel_24112\\3556520258.py:14: DeprecationWarning: sipPyTypeDict() is deprecated, the extension module should use sipPyTypeDictRef() instead\n",
      "  class ObjectSegmentationApp(QtWidgets.QWidget):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "1/1: 0... Failed to read images from 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 54\u001b[0m, in \u001b[0;36mObjectSegmentationApp.segment_objects\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n-seg.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(source\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, show\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 54\u001b[0m \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m results:\n\u001b[0;32m     55\u001b[0m      \u001b[39mprint\u001b[39m(r\u001b[39m.\u001b[39mkeypoints)\n\u001b[0;32m     57\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mSegmented Image\u001b[39m\u001b[39m'\u001b[39m, r\u001b[39m.\u001b[39mkeypoints)\n",
      "File \u001b[1;32mc:\\Users\\Carrington\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[39m=\u001b[39m gen\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[39m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carrington\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\predictor.py:240\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msetup_model(model)\n\u001b[0;32m    238\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:  \u001b[39m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     \u001b[39m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_source(source \u001b[39mif\u001b[39;49;00m source \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49msource)\n\u001b[0;32m    242\u001b[0m     \u001b[39m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave_txt:\n",
      "File \u001b[1;32mc:\\Users\\Carrington\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\engine\\predictor.py:215\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz \u001b[39m=\u001b[39m check_imgsz(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mimgsz, stride\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mstride, min_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# check image size\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mmodel, \u001b[39m'\u001b[39m\u001b[39mtransforms\u001b[39m\u001b[39m'\u001b[39m, classify_transforms(\n\u001b[0;32m    214\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz[\u001b[39m0\u001b[39m])) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mtask \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mclassify\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 215\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset \u001b[39m=\u001b[39m load_inference_source(source\u001b[39m=\u001b[39;49msource,\n\u001b[0;32m    216\u001b[0m                                      imgsz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimgsz,\n\u001b[0;32m    217\u001b[0m                                      vid_stride\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mvid_stride,\n\u001b[0;32m    218\u001b[0m                                      buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mstream_buffer)\n\u001b[0;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msource_type \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39msource_type\n\u001b[0;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mTrue\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# streams\u001b[39;00m\n\u001b[0;32m    221\u001b[0m                                           \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset) \u001b[39m>\u001b[39m \u001b[39m1000\u001b[39m \u001b[39mor\u001b[39;00m  \u001b[39m# images\u001b[39;00m\n\u001b[0;32m    222\u001b[0m                                           \u001b[39many\u001b[39m(\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m'\u001b[39m\u001b[39mvideo_flag\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mFalse\u001b[39;00m]))):  \u001b[39m# videos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Carrington\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\data\\build.py:166\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    164\u001b[0m     dataset \u001b[39m=\u001b[39m source\n\u001b[0;32m    165\u001b[0m \u001b[39melif\u001b[39;00m webcam:\n\u001b[1;32m--> 166\u001b[0m     dataset \u001b[39m=\u001b[39m LoadStreams(source, imgsz\u001b[39m=\u001b[39;49mimgsz, vid_stride\u001b[39m=\u001b[39;49mvid_stride, buffer\u001b[39m=\u001b[39;49mbuffer)\n\u001b[0;32m    167\u001b[0m \u001b[39melif\u001b[39;00m screenshot:\n\u001b[0;32m    168\u001b[0m     dataset \u001b[39m=\u001b[39m LoadScreenshots(source, imgsz\u001b[39m=\u001b[39mimgsz)\n",
      "File \u001b[1;32mc:\\Users\\Carrington\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\data\\loaders.py:102\u001b[0m, in \u001b[0;36mLoadStreams.__init__\u001b[1;34m(self, sources, imgsz, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    100\u001b[0m success, im \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaps[i]\u001b[39m.\u001b[39mread()  \u001b[39m# guarantee first frame\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m success \u001b[39mor\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mst\u001b[39m}\u001b[39;00m\u001b[39mFailed to read images from \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgs[i]\u001b[39m.\u001b[39mappend(im)\n\u001b[0;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape[i] \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;31mConnectionError\u001b[0m: 1/1: 0... Failed to read images from 0"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carrington\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "#from yolov5.detect import detect\n",
    "from ultralytics import YOLO\n",
    "import sys\n",
    "from PyQt5 import QtWidgets\n",
    "import cv2  # OpenCV for object segmentation\n",
    "\n",
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "\n",
    "\n",
    "\n",
    "class ObjectSegmentationApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Object Segmentation\")\n",
    "        self.setGeometry(100, 100, 600, 400)\n",
    "        self.background_label = QtWidgets.QLabel(self)\n",
    "        self.background_label.setGeometry(100, 100, 600, 400)  # Set the label size to match the window\n",
    "        self.set_background_image(r\"C:\\Users\\Carrington\\Data_visualization_using_PyQt_designer\\nexo_blog_okladka_1920x500_proof-of-concept.png\")        \n",
    "        self.image_label = QtWidgets.QLabel(self)\n",
    "        self.image_label.setGeometry(100, 100, 600, 400)\n",
    "        self.image_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.image_label.setScaledContents(True)  \n",
    "\n",
    "        self.file_dialog = QtWidgets.QFileDialog()\n",
    "        self.file_dialog.setFileMode(QtWidgets.QFileDialog.ExistingFile)\n",
    "        self.file_dialog.setNameFilter(\"Image Files (*.jpg *.png *.bmp)\")\n",
    "\n",
    "        #self.load_button = QtWidgets.QPushButton(\"Load Image\")\n",
    "        #self.load_button.clicked.connect(self.load_image)\n",
    "\n",
    "        self.segment_button = QtWidgets.QPushButton(\"segment objects\")\n",
    "        self.segment_button.clicked.connect(self.segment_objects)\n",
    "        self.segment_button.setEnabled(True)\n",
    "\n",
    "        layout = QtWidgets.QVBoxLayout()\n",
    "       # layout.addWidget(self.load_button)\n",
    "        layout.addWidget(self.segment_button)\n",
    "        self.setLayout(layout)\n",
    "\n",
    "    def set_background_image(self, image_path):\n",
    "        # Load the background image\n",
    "        pixmap = QtGui.QPixmap(image_path)\n",
    "        # Scale the image to fit the window size\n",
    "        scaled_pixmap = pixmap.scaled(self.size(), aspectRatioMode=QtCore.Qt.KeepAspectRatioByExpanding)\n",
    "        # Set the pixmap as the background of the label\n",
    "        self.background_label.setPixmap(scaled_pixmap)\n",
    "\n",
    "    def segment_objects(self):\n",
    "        model= YOLO('yolov8n-seg.pt')\n",
    "        results = model.predict(source=0, stream=True, show=True)\n",
    "        for r in results:\n",
    "             print(r.keypoints)\n",
    "\n",
    "        cv2.imshow('Segmented Image', r.keypoints)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    segmentation_app = ObjectSegmentationApp()\n",
    "    segmentation_app.show()\n",
    "    sys.exit(app.exec_())\n",
    "\n",
    "    for r in results:\n",
    "        print(r.keypoints)\n",
    "   # results = object_detection_yolov5(image_path)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def object_detection_yolov5(image_path):\n",
    "    model= YOLO('yolov8n-seg.pt')\n",
    "    results = model.predict(source=0, stream=True, show=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from PyQt5 import QtWidgets, QtGui, QtCore\n",
    "\n",
    "class ImageDetectionApp(QtWidgets.QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Image Detection with Background Picture\")\n",
    "        self.setGeometry(100, 100, 800, 600)\n",
    "\n",
    "        # Set up a QLabel for the background image\n",
    "        self.background_label = QtWidgets.QLabel(self)\n",
    "        self.background_label.setGeometry(100, 100, 800, 600)  # Set the label size to match the window\n",
    "\n",
    "        # Set a background image\n",
    "        self.set_background_image(r\"C:\\Users\\Carrington\\Data_visualization_using_PyQt_designer\\sensor_data.png\")  # Replace with your background image path\n",
    "\n",
    "        # Set up another QLabel for displaying the loaded image\n",
    "        self.image_label = QtWidgets.QLabel(self)\n",
    "        self.image_label.setGeometry(100, 100, 600, 400)\n",
    "        self.image_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.image_label.setScaledContents(True)\n",
    "\n",
    "        # Load Image Button\n",
    "        self.load_image_button = QtWidgets.QPushButton(\"Load Image\", self)\n",
    "        self.load_image_button.setGeometry(300, 520, 200, 40)\n",
    "        self.load_image_button.clicked.connect(self.load_image)\n",
    "\n",
    "    def set_background_image(self, image_path):\n",
    "        # Load the background image\n",
    "        pixmap = QtGui.QPixmap(image_path)\n",
    "        # Scale the image to fit the window size\n",
    "        scaled_pixmap = pixmap.scaled(self.size(), aspectRatioMode=QtCore.Qt.KeepAspectRatioByExpanding)\n",
    "        # Set the pixmap as the background of the label\n",
    "        self.background_label.setPixmap(scaled_pixmap)\n",
    "\n",
    "    def load_image(self):\n",
    "        file_dialog = QtWidgets.QFileDialog()\n",
    "        file_dialog.setNameFilter(\"Image Files (*.jpg *.png *.bmp)\")\n",
    "        if file_dialog.exec_():\n",
    "            file_paths = file_dialog.selectedFiles()\n",
    "            if file_paths:\n",
    "                image_path = file_paths[0]\n",
    "                pixmap = QtGui.QPixmap(image_path)\n",
    "                self.image_label.setPixmap(pixmap)\n",
    "                self.image_label.adjustSize()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    window = ImageDetectionApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
